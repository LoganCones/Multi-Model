# Load required libraries
library(randomForest)   # For randomForest algorithm
library(xgboost)        # For XGBoost algorithm
# ... other libraries for other algorithms you choose

# Step 1: Data Preprocessing
# Assuming your data is loaded into 'data' and your binary outcome is in 'outcome'
# Split the data into training and testing sets
set.seed(42)  # For reproducibility
train_index <- createDataPartition(data$outcome, p = 0.7, list = FALSE)
train_data <- data[train_index, ]
test_data <- data[-train_index, ]

# Step 2: Choose Algorithms
algorithms <- c("glm", "xgboost", "svmRadial")

# Step 3: Model Training
models <- list()

# Logistic Regression
models[["Logistic Regression"]] <- train(
  outcome ~ .,
  data = train_data,
  method = "glm",
  trControl = trainControl(method = "cv", number = 5)
)

# Random Forest
models[["Random Forest"]] <- randomForest(
  x = train_data[, -which(names(train_data) == "outcome")],
  y = train_data$outcome,
  importance = TRUE
)

# XGBoost
models[["XGBoost"]] <- xgboost(
  data = as.matrix(train_data[,-which(names(train_data)=="outcome")]), 
  label = train_data$outcome,
  nrounds = 10,
  verbose = 0
)

# SVM
models[["SVM"]] <- train(
  outcome ~ .,
  data = train_data,
  method = "svmRadial",
  trControl = trainControl(method = "cv", number = 5)
)

# Step 4: Model Evaluation
accuracy <- sapply(models, function(model) {
  if (names(model)[1] == "XGBoost") {
    # For XGBoost, use DMatrix for prediction
    dtest <- xgb.DMatrix(data = as.matrix(test_data[,-which(names(test_data)=="outcome")]))
    pred <- predict(model, dtest)
  } else {
    # For other models, use predict function directly
    pred <- predict(model, newdata = test_data)
  }
  mean(pred == test_data$outcome)
})

# Step 5: Feature Importance (for Random Forest)
rf_feature_importance <- importance(models[["Random Forest"]])$importance

# Step 6: Logistic Regression and SVM
logreg_variable_importance <- abs(coef(models[["Logistic Regression"]]))
svm_variable_importance <- abs(coef(models[["SVM"]]))

# Step 7: Create tables for variable importance
logreg_table <- data.frame(
  Variable = names(coef(models[["Logistic Regression"]])),
  Importance = logreg_variable_importance
)

svm_table <- data.frame(
  Variable = names(coef(models[["SVM"]])),
  Importance = svm_variable_importance
)

# Step 8: Merge Random Forest and XGBoost variable importance into tables
rf_table <- data.frame(
  Variable = rownames(rf_feature_importance),
  Importance = rf_feature_importance[, 1]
)

xgb_table <- data.frame(
  Variable = names(models[["XGBoost"]].bst$feature.importance),
  Importance = models[["XGBoost"]].bst$feature.importance
)

# Step 9: Combine all the tables
all_variable_importance_table <- rbind(
  logreg_table,
  rf_table,
  xgb_table,
  svm_table
)

# Step 10: Create a table with accuracy results
results_table <- data.frame(
  Model = names(models),
  Accuracy = accuracy
)

# Step 11: Print the combined table sorted by importance
sorted_table <- all_variable_importance_table[order(-all_variable_importance_table$Importance), ]
print(sorted_table)

# Step 12: Print the accuracy results table
print(results_table)
