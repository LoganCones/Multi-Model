# Load required libraries
library(caret)
library(randomForest)   # For randomForest algorithm
library(xgboost)        # For XGBoost algorithm
# ... other libraries for other algorithms you choose

# Step 1: Data Preprocessing
# Assuming your data is loaded into 'data' and your binary outcome is in 'outcome'
# Split the data into training and testing sets
set.seed(42)  # For reproducibility
train_index <- createDataPartition(data$outcome, p = 0.7, list = FALSE)
train_data <- data[train_index, ]
test_data <- data[-train_index, ]

# Step 2: Choose Algorithms
algorithms <- c("glm", "randomForest", "xgbTree", "svmRadial")

# Step 3: Model Training
models <- lapply(algorithms, function(algo) {
  train(
    outcome ~ .,                 # Replace . with the appropriate formula
    data = train_data,
    method = algo,               # Algorithm name from 'algorithms'
    trControl = trainControl(method = "cv", number = 5)   # Cross-validation for tuning
  )
})

# Step 4: Model Evaluation
accuracy <- sapply(models, function(model) {
  pred <- predict(model, newdata = test_data)
  mean(pred == test_data$outcome)
})

# Step 5: Feature Importance (for Random Forest and XGBoost)
rf_model <- models[[2]]  # Assuming Random Forest is the second algorithm in the list
rf_feature_importance <- varImp(rf_model)   # Random Forest feature importance
xgb_model <- models[[3]] # Assuming XGBoost is the third algorithm in the list
xgb_feature_importance <- xgb.importance(model = xgb_model)   # XGBoost feature importance

# Step 6: Logistic Regression and SVM
logreg_model <- models[[1]]   # Assuming Logistic Regression is the first algorithm in the list
svm_model <- models[[4]]      # Assuming SVM is the fourth algorithm in the list

# Step 7: Calculate variable importance for Logistic Regression and SVM
logreg_variable_importance <- abs(coef(logreg_model))
svm_variable_importance <- abs(coef(svm_model))

# Step 8: Create tables for variable importance
logreg_table <- data.frame(
  Variable = names(coef(logreg_model)),
  Importance = logreg_variable_importance
)

svm_table <- data.frame(
  Variable = names(coef(svm_model)),
  Importance = svm_variable_importance
)

# Step 9: Merge Random Forest and XGBoost variable importance into tables
rf_table <- data.frame(
  Variable = rownames(rf_feature_importance),
  Importance = rf_feature_importance$Overall
)

xgb_table <- data.frame(
  Variable = rownames(xgb_feature_importance),
  Importance = xgb_feature_importance$Feature.Importance
)

# Step 10: Combine all the tables
all_variable_importance_table <- rbind(
  logreg_table,
  rf_table,
  xgb_table,
  svm_table
)

# Step 11: Create a table with accuracy results
results_table <- data.frame(
  Model = c("Logistic Regression", "Random Forest", "XGBoost", "SVM"),
  Accuracy = accuracy
)

# Step 12: Print the combined table sorted by importance
sorted_table <- all_variable_importance_table[order(-all_variable_importance_table$Importance), ]
print(sorted_table)

# Step 13: Print the accuracy results table
print(results_table)
